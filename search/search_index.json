{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Why are we building OLM v1?","text":"<p>Operator Lifecycle Manager's mission has been to manage the lifecycle of cluster extensions centrally and declaratively on Kubernetes clusters. Its purpose has always been to make installing, running, and updating functional extensions to the cluster easy, safe, and reproducible for cluster administrators and PaaS administrators, throughout the lifecycle of the underlying cluster. </p> <p>OLM v0 was focused on providing unique support for these specific needs for a particular type of cluster extension, which have been coined as operators.  Operators are classified as one or more Kubernetes controllers, shipping with one or more API extensions (CustomResourceDefinitions) to provide additional functionality to the cluster. After running OLM v0 in production clusters for a number of years, it became apparent that there's an appetite to deviate from this coupling of CRDs and controllers, to encompass the lifecycling of extensions that are not just operators.</p> <p>OLM has been helping to define lifecycles for these extensions in which the extensions</p> <ul> <li>get installed, potentially causing other extensions to be installed as well as dependencies</li> <li>get customized with the help of customizable configuration at runtime </li> <li>get upgraded to newer version/s following upgrade paths defined by extension developers </li> <li>and finally, get decommissioned and removed.</li> </ul> <p>In the dependency model, extensions can rely on each other for required services that are out of scope of the primary purpose of an extension, allowing each extension to focus on a specific purpose. </p> <p>OLM also prevents conflicting extensions from running on the cluster, either with conflicting dependency constraints or conflicts in ownership of services provided via APIs. Since cluster extensions need to be supported with an enterprise-grade product lifecycle, there has been a growing need for allowing extension authors to limit installation and upgrade of their extension by specifying additional environmental constraints as dependencies, primarily to align with what was tested by the extension author's QE processes. In other words, there is an evergrowing ask for OLM to allow the author to enforce these support limitations in the form of additional constraints specified by extension authors in their packaging for OLM.</p> <p>During their lifecycle on the cluster, OLM also manages the permissions and capabilities extensions have on the cluster as well as the permission and access tenants on the cluster have to the extensions. This is done using the Kubernetes RBAC system, in combination with tenant isolation using Kubernetes namespaces. While the interaction surface of the extensions is solely composed of Kubernetes APIs the extensions define, there is an acute need to rethink the way tenant(i.e consumers of extensions) isolation is achieved. The ask from OLM, is to provide tenant isolation in a more intuitive way than is implemented in OLM v0</p> <p>OLM also defines a packaging model in which catalogs of extensions, usually containing the entire version history of each extension, are made available to clusters for cluster users to browse and select from. While these catalogs have so far been packaged and shipped as container images, there is a growing appetite to allow more ways of packaging and shipping these catalogs, besides also simplifying the building process of these catalogs, which so far have been very costly. The effort to bring down the cost was kicked off in OLM v0 with conversion of the underlying datastore for catalog metadata to File-based Catalogs, with more effort being invested to slim down the process in v1. Via new versions of extensions delivered with this new packaging system, OLM will be able to apply updates to existing running extensions on the cluster in a way where the integrity of the cluster is maintained and constraints and dependencies are kept satisfied.</p> <p>For a detailed writeup of OLM v1 requirements, please read the Product Requirements Documentation</p>"},{"location":"#the-olm-community","title":"The OLM community","text":"<p>The OLM v1 project is being tracked in a GitHub project</p> <p>You can reach out to the OLM community for feedbacks/discussions/contributions in the following channels:</p> <ul> <li>Kubernetes slack channel: #olm-dev</li> <li>Operator Framework on Google Groups</li> </ul>"},{"location":"components/","title":"Components","text":"<p>OLM v1 is composed of various component projects: </p> <ul> <li> <p>operator-controller: operator-controller is the central component of OLM v1, that consumes all of the components below to extend Kubernetes to allows users to install, and manage the lifecycle of other extensions</p> </li> <li> <p>deppy: Deppy is a Kubernetes API that runs on- or off-cluster for resolving constraints over catalogs of RukPak bundles. Deppy is part of the next iteration of OLM and was first introduced here. The initial goal of the project is to remove the dependency manager from the Operator Lifecycle Manager (OLM) and make it its own generic component.</p> </li> <li> <p>catalogD: Catalogd is a Kubernetes extension that unpacks file-based catalog (FBC) content that is packaged and shipped in container images, for consumption by clients on-clusters (unpacking from other sources, like git repos, OCI artifacts etc, are in the roadmap for catalogD). As component of the Operator Lifecycle Manager (OLM) v1 microservices architecture, catalogD hosts metadata for Kubernetes extensions packaged by the authors of the extensions, as a result helping customers discover installable content.</p> </li> </ul>"},{"location":"olmv1_overview/","title":"OLMv1 Overview","text":""},{"location":"olmv1_overview/#what-wont-olmv1-do-that-olmv0-did","title":"What won't OLMv1 do that OLMv0 did?","text":"<p>TL;DR: OLMv1 cannot feasibly support multi-tenancy or any feature that assumes multi-tenancy. All multi-tenancy features end up falling over because of the global API system of Kubernetes. While this short conclusion may be unsatisfying, the reasons are complex and intertwined.</p>"},{"location":"olmv1_overview/#historical-context","title":"Historical Context","text":"<p>Nearly every active contributor in the Operator Framework project contributed to design explorations and prototypes over an entire year. For each of these design explorations, there are complex webs of features and assumptions that are necessary to understand the context that ultimately led to a conclusion of infeasibility.</p> <p>Here is a sampling of some of the ideas we explored: - [WIP] OLM v1's approach to multi-tenancy - OLMv1 Multi-tenancy Brainstorming</p>"},{"location":"olmv1_overview/#watched-namespaces-cannot-be-configured-in-a-first-class-api","title":"Watched namespaces cannot be configured in a first-class API","text":"<p>OLMv1 will not have a first-class API for configuring the namespaces that a controller will watch.</p> <p>Kubernetes APIs are global. Kubernetes is designed with the assumption that a controller WILL reconcile an object no matter where it is in the cluster.</p> <p>However, Kubernetes does not assume that a controller will be successful when it reconciles an object.</p> <p>The Kubernetes design assumptions are: - CRDs and their controllers are trusted cluster extensions. - If an object for an API exists a controller WILL reconcile it, no matter where it is in the cluster.</p> <p>OLMv1 will make the same assumption that Kubernetes does and that users of Kubernetes APIs do. That is: If a user has RBAC to create an object in the cluster, they can expect that a controller exists that will reconcile that object. If this assumption does not hold, it will be considered a configuration issue, not an OLMv1 bug.</p> <p>This means that it is a best practice to implement and configure controllers to have cluster-wide permission to read and update the status of their primary APIs. It does not mean that a controller needs cluster-wide access to read/write secondary APIs. If a controller can update the status of its primary APIs, it can tell users when it lacks permission to act on secondary APIs.</p>"},{"location":"olmv1_overview/#dependencies-based-on-watched-namespaces","title":"Dependencies based on watched namespaces","text":"<p>Since there will be no first-class support for configuration of watched namespaces, OLMv1 cannot resolve dependencies among bundles based on where controllers are watching.</p> <p>However, not all bundle constraints are based on dependencies among bundles from different packages. OLMv1 will be able to support constraints against cluster state. For example, OLMv1 could support a \u201ckubernetesVersionRange\u201d constraint that blocks installation of a bundle if the current kubernetes cluster version does not fall into the specified range.</p>"},{"location":"olmv1_overview/#background","title":"Background","text":"<p>For packages that specify API-based dependencies, OLMv0\u2019s dependency checker knows which controllers are watching which namespaces. While OLMv1 will have awareness of which APIs are present on a cluster (via the discovery API), it will not know which namespaces are being watched for reconciliation of those APIs. Therefore dependency resolution based solely on API availability would only work in cases where controllers are configured to watch all namespaces.</p> <p>For packages that specify package-based dependencies, OLMv0\u2019s dependency checker again knows which controllers are watching which namespaces. This case is challenging for a variety of reasons: 1. How would a dependency resolver know which extensions were installed (let alone which extensions were watching which namespaces)? If a user is running the resolver, they would be blind to an installed extension that is watching their namespace if they don\u2019t have permission to list extensions in the installation namespace. If a controller is running the resolver, then it might leak information to a user about installed extensions that the user is not otherwise entitled to know. 2. Even if (1) could be overcome, the lack of awareness of watched namespaces means that the resolver would have to make assumptions. If only one controller is installed, is it watching the right set of namespaces to meet the constraint? If multiple controllers are installed, are any of them watching the right set of namespaces? Without knowing the watched namespaces of the parent and child controllers, a correct dependency resolver implementation is not possible to implement.</p> <p>Note that regardless of the ability of OLMv1 to perform dependency resolution (now or in the future), OLMv1 will not automatically install a missing dependency when a user requests an operator. The primary reasoning is that OLMv1 will err on the side of predictability and cluster-administrator awareness.</p>"},{"location":"olmv1_overview/#watch-namespace-aware-operator-discoverability","title":"\"Watch namespace\"-aware operator discoverability","text":"<p>When operators add APIs to a cluster, these APIs are globally visible. As stated before, there is an assumption in this design that a controller will reconcile an object of that API anywhere it exists in the cluster.</p> <p>Therefore, the API discoverability story boils down to answering this question for the user: \u201cWhat APIs do I have access to in a given namespace?\u201d Fortunately, built-in APIs exist to answer this question: Kubernetes Discovery, SelfSubjectRulesReview (SSRR), and SelfSubjectAccessReview (SSAR).</p> <p>However, helping users discover which actual controllers will reconcile those APIs is not possible unless OLMv1 knows which namespaces those controllers are watching.</p> <p>Any solution here would be unaware of where a controller is actually watching and could only know \u201cis there a controller installed that provides an implementation of this API?\u201d. However even knowledge of a controller installation is not certain. Any user can use the discovery, SSRR, and SSAR. Not all users can list all Extensions (see User discovery of \u201cavailable\u201d APIs).</p>"},{"location":"olmv1_overview/#what-does-this-mean-for-multi-tenancy","title":"What does this mean for multi-tenancy?","text":"<p>The multi-tenancy promises that OLMv0 made were false promises. Kubernetes is not multi-tenant with respect to management of APIs (because APIs are global). Any promise that OLMv0 has around multi-tenancy evaporates when true tenant isolation attempts are made, and any attempt to fix a broken promise is actually just a bandaid on an already broken assumption.</p> <p>So where do we go from here? There are multiple solutions that do not involve OLM implementing full multi-tenancy support, some or all of which can be explored. 1. Customers transition to a control plane per tenant 2. Extension authors update their operators to support customers\u2019 multi-tenancy use cases 3. Extension authors with \u201csimple\u201d lifecycling concerns transition to other packaging and deployment strategies (e.g. helm charts)</p>"},{"location":"olmv1_overview/#single-tenant-control-planes","title":"Single-tenant control planes","text":"<p>One choice for customers would be to adopt low-overhead single-tenant control planes in which every tenant can have full control over their APIs and controllers and be truly isolated (at the control plane layer at least) from other tenants. With this option, the things OLMv1 cannot do (listed above) are irrelevant, because the purpose of all of those features is to support multi-tenant control planes in OLM.</p> <p>The Kubernetes multi-tenancy docs contain a good overview of the options in this space. Kubernetes vendors may also have their own virtual control plane implementations.</p>"},{"location":"olmv1_overview/#shift-multi-tenant-responsibility-to-operators","title":"Shift multi-tenant responsibility to operators","text":"<p>There is a set of operators that both (a) provide fully namespace-scoped workload-style operands and that (b) provide a large amount of value to their users for advanced features like backup and migration. For these operators, the Operator Framework program would suggest that they shift toward supporting multi-tenancy directly. That would involve: 1. Taking extreme care to avoid API breaking changes. 2. Supporting multiple versions of their operands in a single version of the operator (if required by users in multi-tenant clusters). 3. Maintaining support for versioned operands for the same period of time that the operator is supported for a given cluster version. 4. Completely avoiding global configuration. Each tenant should be able to provide their configuration separately.</p>"},{"location":"olmv1_overview/#operator-authors-ship-controllers-outside-of-olm","title":"Operator authors ship controllers outside of OLM","text":"<p>Some projects have been successful delivering and supporting their operator on Kubernetes, but outside of OLM, for example with helm-packaged operators. On this path, individual layered project teams have more flexibility in solving lifecycling problems for their users because they are unencumbered by OLM\u2019s opinions. However the tradeoff is that those project teams and their users take on responsibility and accountability for safe upgrades, automation, and multi-tenant architectures. With OLMv1 no longer attempting to support multi-tenancy in a first-class way, these tradeoffs change and project teams may decide that a different approach is necessary.</p> <p>This path does not necessarily mean a scattering of content in various places. It would still be possible to provide customers with a marketplace of content (e.g. see https://artifacthub.io/).</p>"},{"location":"olmv1_overview/#authors-of-simple-operators-ship-their-workload-without-an-operator","title":"Authors of \"simple\" operators ship their workload without an operator","text":"<p>Another direction is to just stop shipping content via an operator. The operator pattern is particularly well-suited for applications that require complex upgrade logic, that need to convert declarative intent to imperative action, or that require sophisticated health monitoring logic and feedback. But a sizable portion of the OperatorHub catalog contain operators that are not actually taking advantage of the benefits of the operator pattern and are instead a simple wrapper around the workload, which is where the real value is.</p> <p>Using the Operator Capability Levels as a rubric, operators that fall into Level 1 and some that fall into Level 2 are not making full use of the operator pattern. If content authors had the choice to ship their content without also shipping an operator that performs simple installation and upgrades, many supporting these Level 1 and Level 2 operators might make that choice to decrease their overall maintenance and support burden while losing very little in terms of value to their customers.</p>"},{"location":"olmv1_overview/#what-will-olm-do-that-a-generic-package-manager-doesnt","title":"What will OLM do that a generic package manager doesn't?","text":"<p>OLM will provide multiple features that are absent in generic package managers. Some items listed below are already implemented, while others may be planned for the future.</p>"},{"location":"olmv1_overview/#upgrade-controls","title":"Upgrade controls","text":"<p>An operator author can control the upgrade flow by specifying supported upgrade paths from one version to another. Or, they could use semantic versioning (semver) - this is fully supported by OLM too.</p> <p>A user can see the author\u2019s supplied upgrade information.</p> <p>While these features might seem standard in package managers, they are fairly unique in the Kubernetes ecosystem. Helm, for example, doesn\u2019t have any features that help users stay on supported upgrade paths.</p>"},{"location":"olmv1_overview/#on-cluster-component-for-automated-upgrades-health-monitoring","title":"On-cluster component for automated upgrades, health monitoring","text":"<p>OLM automatically upgrades an operator to the latest acceptable matching version whenever a new matching version appears in a catalog, assuming the user has enabled this for their operator.</p> <p>OLM constantly monitors the state of all on-cluster resources for all the operators it manages, reporting the health in aggregate on each operator.</p>"},{"location":"olmv1_overview/#crd-upgrade-safety-checks","title":"CRD Upgrade Safety Checks","text":"<p>Before OLM upgrades a CRD, OLM performs a set of safety checks to identify any changes that potentially would have negative impacts, such as: - data loss - incompatible schema changes</p> <p>These checks may not be a guarantee that an upgrade is safe; instead, they are intended to provide an early warning sign for identifiable incompatibilities. False positives (OLMv1 claims a breaking change when there is none) and false negatives (a breaking change makes it through the check without being caught) are possible, at least while the OLMv1 team iterates on this feature.</p>"},{"location":"olmv1_overview/#user-permissions-management","title":"User permissions management","text":"<p>Operators typically provide at least one new API, and often multiple. While operator authors know the APIs they\u2019re providing, users installing operators won\u2019t necessarily have this same knowledge. OLM will make it easy to grant permissions to operator-provided APIs to users/groups in various namespaces, but any automation (which would be client-side only) or UX provided by OLM related to user permissions management will be unable to automatically account for watch namespace configurations. See Watched namespaces cannot be configured in a first class API</p> <p>Also note that user permission management does not unlock operator discoverability (only API discoverability). See \u201cWatch namespace\u201d-aware operator discoverability for more details.</p>"},{"location":"olmv1_overview/#user-discovery-of-available-apis","title":"User discovery of \u201cavailable\u201d APIs","text":"<p>In the future, the Operator Framework team could explore building an API similar to SelfSubjectAccessReview and SelfSubjectRulesReview that answers the question: \u201cWhat is the public metadata of all of the extensions that are installed on the cluster that provide APIs that I have permission for in namespace X?\u201d</p> <p>One solution would be to join \u201cinstalled extensions with user permissions\u201d. If an installed extension provides an API that a user has RBAC permission for, that extension would be considered available to that user in that scope. This solution would not be foolproof: it makes the (reasonable) assumption that an administrator only configures RBAC for a user in a namespace where a controller is reconciling that object. If an administrator gives a user RBAC access to an API without also configuring that controller to watch the namespace that they have access to, the discovery solution would report an available extension, but then nothing would actually reconcile the object they create.</p> <p>This solution would tell users about API-only and API+controller bundles that are installed. It would not tell users about controller-only bundles, because they do not include APIs.</p> <p>Other similar API-centric solutions could be explored as well. For example, pursuing enhancements to OLMv1 or core Kubernetes related to API metadata and/or grouping.</p> <p>A key note here is that controller-specific metadata like the version of the controller that will reconcile the object in a certain namespace is not necessary for discovery. Discovery is primarily about driving user flows around presenting information and example usage of a group of APIs such that CLIs and UIs can provide rich experiences around interactions with available APIs.</p>"},{"location":"olmv1_overview/#approach","title":"Approach","text":"<p>We will adhere to the following tenets in our approach for the design and implementation of OLMv1</p>"},{"location":"olmv1_overview/#do-not-fight-kubernetes","title":"Do not fight Kubernetes","text":"<p>One of the key features of cloud-native applications/extensions/operators is that they typically come with a Kubernetes-based API (e.g. CRD) and a controller that reconciles instances of that API. In Kubernetes, API registration is cluster-scoped. It is not possible to register different APIs in different namespaces. Instances of an API can be cluster- or namespace-scoped. All APIs are global (they can be invoked/accessed regardless of namespace). For cluster-scoped APIs, the names of their instances must be unique. For example, it\u2019s possible to have Nodes named \u201cone\u201d and \u201ctwo\u201d, but it\u2019s not possible to have multiple Nodes named \u201ctwo\u201d. For namespace-scoped APIs, the names of their instances must be unique per namespace. The following illustrates this for ConfigMaps, a namespace-scoped API:</p> <p>Allowed - Namespace: test, name: my-configmap - Namespace: other, name: my-configmap</p> <p>Disallowed - Namespace: test, name: my-configmap - Namespace: test, name: my-configmap</p> <p>In cases where OLMv0 decides that joint ownership of CRDs will not impact different tenants, OLMv0 allows multiple installations of bundles that include the same named CRD, and OLMv0 itself manages the CRD lifecycle. This has security implications because it requires OLMv0 to act as a deputy, but it also pits OLM against the limitations of the Kubernetes API. OLMv0 promises that different versions of an operator can be installed in the cluster for use by different tenants without tenants being affected by each other. This is not a promise OLM can make because it is not possible to have multiple versions of the same CRD present on a cluster for different tenants.</p> <p>In OLMv1, we will not design the core APIs and controllers around this promise. Instead, we will build an API where ownership of installed objects is not shared. Managed objects are owned by exactly one extension.</p> <p>This pattern is generic, aligns with the Kubernetes API, and makes multi-tenancy a possibility, but not a guarantee or core concept. We will explore the implications of this design on existing OLMv0 registry+v1 bundles as part of the larger v0 to v1 migration design. For net new content, operator authors that intend multiple installations of operator on the same cluster would need to package their components to account for this ownership rule. Generally, this would entail separation along these lines: - CRDs, conversion webhook workloads, and admission webhook configurations and workloads, APIServices and workloads. - Controller workloads, service accounts, RBAC, etc.</p> <p>OLMv1 will include primitives (e.g. templating) to make it possible to have multiple non-conflicting installations of bundles.</p> <p>However it should be noted that the purpose of these primitives is not to enable multi-tenancy. It is to enable administrators to provide configuration for the installation of an extension. The fact that operators can be packaged as separate bundles and parameterized in a way that permits multiple controller installations is incidental, and not something that OLMv1 will encourage or promote.</p>"},{"location":"olmv1_overview/#make-olm-secure-by-default","title":"Make OLM secure by default","text":"<p>OLMv0 runs as cluster-admin, which is a security concern. OLMv0 has optional security controls for operator installations via the OperatorGroup, which allows a user with permission to create or update them to also set a ServiceAccount that will be used for authorization purposes on operator installations and upgrades in that namespace. If a ServiceAccount is not explicitly specified, OLM\u2019s cluster-admin credentials are used. Another avenue that cluster administrators have is to lock down permissions and usage of the CatalogSource API, disable default catalogs, and provide tenants with custom vetted catalogs. However if a cluster admin is not aware of these options, the default configuration of a cluster results in users with permission to create a Subscription in namespaces that contain an OperatorGroup effectively have cluster-admin, because OLMv0 has unlimited permissions to install any bundle available in the default catalogs and the default community catalog is not vetted for limited RBAC. Because OLMv0 is used to install more RBAC and run arbitrary workloads, there are numerous potential vectors that attackers could exploit. While there are no known exploits and there has not been any specific concern reported from customers, we believe CNCF\u2019s reputation rest on secure cloud-native software and that this is a non-negotiable area to improve.</p> <p>To make OLM secure by default: - OLMv1 will not be granted cluster admin permissions. Instead it will require service accounts provided by users to actually install, upgrade, and delete content. In addition to the security this provides, it also fulfills one of OLM\u2019s long-standing requirements: halt when bundle upgrades require additional permissions and wait until those permissions are granted. - OLMv1 will use secure communication protocols between all internal components and between itself and its clients.</p>"},{"location":"olmv1_overview/#simple-and-predictable-semantics-for-install-upgrade-and-delete","title":"Simple and predictable semantics for install, upgrade, and delete","text":"<p>OLMv0 has grown into a complex web of functionality that is difficult to understand, even for seasoned Kubernetes veterans.</p> <p>In OLMv1 we will move to GitOps-friendly APIs that allow administrators to rely on their experience with conventional Kubernetes API behavior (declarative, eventually consistent) to manage operator lifecycles.</p> <p>OLMv1 will reduce its API surface down to two primary APIs that represent catalogs of content, and intent for that content to be installed on the cluster.</p> <p>OLMv1 will: - Permit administrators to pin to specific versions, channels, version ranges, or combinations of both. - Permit administrators to pause management of an installation for maintenance or troubleshooting purposes. - Put opinionated guardrails up by default (e.g. follow operator developer-defined upgrade edges). - Give administrators escape hatches to disable any or all of OLMs guardrails. - Delete managed content when a user deletes the OLM object that represents it.</p>"},{"location":"olmv1_overview/#apis-and-behaviors-to-handle-common-controller-patterns","title":"APIs and behaviors to handle common controller patterns","text":"<p>OLMv0 takes an extremely opinionated stance on the contents of the bundles it installs and in the way that operators can be lifecycled. The original designers believed these opinions would keep OLM\u2019s scope limited and that they encompassed best practices for operator lifecycling. Some of these opinions are: - All bundles must include a ClusterServiceVersion, which ostensibly gives operator authors an API that they can use to fully describe how to run the operator, what permissions it requires, what APIs it provides, and what metadata to show to users. - Bundles cannot contain arbitrary objects. OLMv0 needs to have specific handling for each resource that it supports. - Cluster administrators cannot override OLM safety checks around CRD changes or upgrades.</p> <p>OLMv1 will take a slightly different approach: - It will not require bundles to have very specific controller-centric shapes. OLMv1 will happily install a bundle that contains a deployment, service, and ingress or a bundle that contains a single configmap. - However for bundles that do include CRDs, controllers, RBAC, webhooks, and other objects that relate to the behavior of the apiserver, OLM will continue to have opinions and special handling:   - CRD upgrade checks (best effort)   - Specific knowledge and handling of webhooks. - To the extent necessary OLMv1 will include optional controller-centric concepts in its APIs and or CLIs in order to facilitate the most common controller patterns. Examples could include:   - Permission management   - CRD upgrade check policies - OLMv1 will continue to have opinions about upgrade traversals and CRD changes that help users prevent accidental breakage, but it will also allow administrators to disable safeguards and proceed anyway.</p> <p>OLMv0 has some support for automatic upgrades. However administrators cannot control the maximum version for automatic upgrades, and the upgrade policy (manual vs automatic) applies to all operators in a namespace. If any operator\u2019s upgrade policy is manual, all upgrades of all operators in the namespace must be approved manually.</p> <p>OLMv1 will have fine-grained control for version ranges (and pins) and for controlling automatic upgrades for individual operators regardless of the policy of other operators installed in the same namespace.</p>"},{"location":"olmv1_overview/#constraint-checking-but-not-automated-on-cluster-management","title":"Constraint checking (but not automated on-cluster management)","text":"<p>OLMv0 includes support for dependency and constraint checking for many common use cases (e.g. required and provided APIs, required cluster version, required package versions). It also has other constraint APIs that have not gained traction (e.g. CEL expressions and compound constraints). In addition to its somewhat hap-hazard constraint expression support, OLMv0 also automatically installs dependency trees, which has proven problematic in several respects: 1. OLMv0 can resolve existing dependencies from outside the current namespace, but it can only install new dependencies in the current namespace. One scenario where this is problematic is if A depends on B, where A supports only OwnNamespace mode and B supports only AllNamespace mode. In that case, OLMv0\u2019s auto dependency management fails because B cannot be installed in the same namespace as A (assuming the OperatorGroup in that namespace is configured for OwnNamespace operators to work). 2. OLMv0\u2019s logic for choosing a dependency among multiple contenders is confusing and error-prone, and an administrator\u2019s ability to have fine-grained control of upgrades is essentially limited to building and deploying tailor-made catalogs. 3. OLMv0 automatically installs dependencies. The only way for an administrator to avoid this OLMv0 functionality is to fully understand the dependency tree in advance and to then install dependencies from the leaves to the root so that OLMv0 always detects that dependencies are already met. If OLMv0 installs a dependency for you, it does not uninstall it when it is no longer depended upon.</p> <p>OLMv1 will not provide dependency resolution among packages in the catalog (see Dependencies based on watched namespaces)</p> <p>OLMv1 will provide constraint checking based on available cluster state. Constraint checking will be limited to checking whether the existing constraints are met. If so, install proceeds. If not, unmet constraints will be reported and the install/upgrade waits until constraints are met.</p> <p>The Operator Framework team will perform a survey of registry+v1 packages that currently rely on OLMv0\u2019s dependency features and will suggest a solution as part of the overall OLMv0 to OLMv1 migration effort.</p>"},{"location":"olmv1_overview/#client-libraries-and-clis-contribute-to-the-overall-ux","title":"Client libraries and CLIs contribute to the overall UX","text":"<p>OLMv0 has no official client libraries or CLIs that can be used to augment its functionality or provide a more streamlined user experience. The kubectl \"operator\" plugin was developed several years ago, but has never been a focus of the core Operator Framework development team, and has never factored into the overall architecture.</p> <p>OLMv1 will deliver an official CLI (likely by overhauling the kubectl operator plugin) and will use it to meet requirements that are difficult or impossible to implement in a controller, or where an architectural assessment dictates that a client is the better choice. This CLI would automate standard workflows over cluster APIs to facilitate simple administrative actions (e.g. automatically create RBAC and ServiceAccounts necessary for an extension installation as an optional step in the CLI\u2019s extension install experience).</p> <p>The official CLI will provide administrators and users with a UX that covers the most common scenarios users will encounter.</p> <p>The official CLI will explicitly NOT attempt to cover complex scenarios. Maintainers will reject requests to over-complicate the CLI. Users with advanced use cases will be able to directly interact with OLMv1\u2019s on-cluster APIs.</p> <p>The idea is: - On-cluster APIs can be used to manage operators in 100% of cases (assuming bundle content is structured in a compatible way) - The official CLI will cover standard user flows, covering ~80% of use cases. - Third-party or unofficial CLIs will cover the remaining ~20% of use cases.</p> <p>Areas where the official CLI could provide value include: - Catalog interactions (search, list, inspect, etc.) - Standard install/upgrade/delete commands - Upgrade previews - RBAC management - Discovery of available APIs</p>"},{"location":"olmv1_roadmap/","title":"OLM v1 roadmap","text":""},{"location":"olmv1_roadmap/#functional-requirements","title":"Functional Requirements","text":"<p>Priority Rating: 1 highest, 2 medium, 3 lower (e.g. P2 = Medium Priority)</p>"},{"location":"olmv1_roadmap/#f1-extension-catalogs-p1","title":"F1 - Extension catalogs (P1)","text":"<p>The existing OLM concepts around catalogs, packages and channels is to be used as a basis for below functional requirements.</p>"},{"location":"olmv1_roadmap/#f2-extension-catalog-discovery-p2","title":"F2 - Extension catalog discovery (P2)","text":"<p>Unprivileged tenants need to have the ability to discover extensions that are available to install. In particular users need to be able to discover all versions in all channels that an extension package defines in a catalog. The privilege should be given at the discretion of the cluster administrator.</p>"},{"location":"olmv1_roadmap/#f3-dependency-preview-p3","title":"F3 - Dependency preview (P3)","text":"<p>Before extension installation, OLM needs to introspect the dependencies of an extension and present a preview of the resolution result to the user to let the user confirm they are ok with this.</p>"},{"location":"olmv1_roadmap/#f4-dependency-review-p3","title":"F4 - Dependency review (P3)","text":"<p>For installed extensions OLM needs to surface the dependency relationship to other installed extensions and also highlight which other extensions depend on a particular extension so users are informed about the relationships at runtime.</p>"},{"location":"olmv1_roadmap/#f5-permission-preview-p2","title":"F5 - Permission preview (P2)","text":"<p>Before extension installation and updates, OLM needs to allow introspection of the permissions the extension requires on cluster and dependencies APIs. This is especially important during updates when the permission scope increases in which case updates should be blocked until approved (F14).</p>"},{"location":"olmv1_roadmap/#f6-installupdate-preflight-checks-p1","title":"F6 - Install/Update preflight checks (P1)","text":"<p>When installing and updating extensions OLM should carry out a set of preflight checks to prevent installations from going into a failed state as a result of attempting an upgrade or install. Preflight checks should include availability and (if applicable) health of (running) dependencies and any cluster runtime constraints (F19).</p>"},{"location":"olmv1_roadmap/#f7-extension-installation-p1","title":"F7 - Extension installation (P1)","text":"<p>OLM needs a declarative way to install extensions either from a catalog of extensions or directly from an OCI image. Should the installation attempt fail due to unfilled requirements, constraints, preflight checks or dependencies there needs to be an option to force the install. Extensions are cluster-wide singletons, thus they can only be installed once in the cluster and are managed at cluster-scope.</p>"},{"location":"olmv1_roadmap/#f8-semver-based-update-policy-p2","title":"F8 - Semver-based update policy (P2)","text":"<p>OLM should allow users to specify if and when extensions are updated. Manual update policy should include the user explicitly approving an update to be installed. An automatic update policy should allow updates to automatically be applied as soon as they are available and should provide further conditions upon which an update is to be applied. Conditions concern version changes of the extension, specifically: automatic updates on z-streams only, automatic updates on y-streams only, always automatic update. Updates across channels are outside of the update policy.</p>"},{"location":"olmv1_roadmap/#f9-update-notification-p3","title":"F9 - Update notification (P3)","text":"<p>As updates can be made available at any time using OLMs existing over-the-air catalog update capabilities, OLM should provide events / notifications on the platform to notify users about available but not yet approved updates of existing installed extensions, specifically so that graphical consoles can pick them up and visualize them. Automatically applied updates as per F8 should also create notifications.</p>"},{"location":"olmv1_roadmap/#f10-extension-updates-p2","title":"F10 - Extension updates (P2)","text":"<p>As extensions get updated, either automatically or manually, OLM replaces the older version of the extensions with a newer version atomically. Up until any custom code or conversion logic runs, an update should be able to be rolled back (F23). When multiple extensions are updated to satisfy an update request, the update policy of each extension needs to be respected to allow users to pin certain extensions to installed versions or certain types of updates (e.g. z-stream only). It should also be possible to force an update to a certain version, even if there is no direct path as per the graph metadata. Otherwise all versions along the update path should be allowed for selecting by the user as the desired target version.Otherwise all versions on</p>"},{"location":"olmv1_roadmap/#f11-request-approval-flow-for-installs-updates-p2","title":"F11 - Request / Approval Flow for Installs / Updates (P2)","text":"<p>To support multi-tenant environments a request / approval flow is desirable for generally available content within default catalogs. In this model any tenant with enough privilege can discover installable content and can trigger an install request, which can in turn be approved or denied by a more privileged administrative role. Such requests should also have timeouts. Administrators should have the ability to define a list of extensions that are automatically approved at the scope of a namespace. Administrators should be able to get aware of unapproved requested via alerts triggered by the platform.</p>"},{"location":"olmv1_roadmap/#f12-installed-extension-discovery-p1","title":"F12 - Installed extension discovery (P1)","text":"<p>Unprivileged tenants need to be able to discover installed extensions if they are offering services for them to use in their namespace. OLM needs to provide distinct controls for installed extensions which administrators can use to regulate in which namespaces extensions are discoverable by tenants, irrespective of the namespaces in which the extension has permissions on cluster APIs (see F13)</p>"},{"location":"olmv1_roadmap/#f13-extension-permissions-management-p1","title":"F13 - Extension permissions management (P1)","text":"<p>Administrative personas need to be able to configure in which namespaces in the cluster the extension can get the requested permissions the extension author deems required. The control needs to be independent of the controls in F12. Extensions should always be given permissions to update their own APIs (if they define any) to inform users about potential lack of permissions in their namespace.</p>"},{"location":"olmv1_roadmap/#f14-extension-permissions-escalation-checks-p2","title":"F14 - Extension permissions escalation checks (P2)","text":"<p>If, in the course of an update, an extension requests more permissions than the currently installed version, an automatic update is blocked and an administrative persona needs to specifically approve the update by default. The user who installed the extension can opt out of these permission increase checks for the purpose of automation.</p>"},{"location":"olmv1_roadmap/#f15-selective-extension-permissions-grants-p3","title":"F15 - Selective extension permissions grants (P3)","text":"<p>Administrative personas can choose to give extensions only a subset of the permissions it requests. This should be manageable at a per namespace level.</p>"},{"location":"olmv1_roadmap/#f16-extension-removal-p1","title":"F16 - Extension removal (P1)","text":"<p>Administrative personas need to be able to remove an extension including all the content that was part of the original installation bundle. Special handling should be implemented for CRDs, which when not removed, are left behind in a functioning state (i.e.any dependencies on running controllers like conversion webhooks need to be removed). When they are to be removed this can only happen if the user opts into F17. Special care also needs to be taken to allow the extension to perform any clean upon getting a signal to be removed. Components need to be removed in an order that allows the extension to handle a graceful shutdown.</p>"},{"location":"olmv1_roadmap/#f17-extension-cascading-removal-p2","title":"F17 - Extension cascading removal (P2)","text":"<p>OLM needs to be able to cleanly remove an extension entirely, which means deleting CRDs and other resources on the cluster. In particular this means the removal of all custom resource instances of the CRDs to be deleted and all extensions with a hard dependency. A user needs to actively opt-in to this behavior and OLM has to sequence the successful removal of all affected components and the extension itself.</p>"},{"location":"olmv1_roadmap/#f18-standalone-extension-bundle-installation-p2","title":"F18 - Standalone extension bundle installation (P2)","text":"<p>For local development OLM should allow the installation of an extension by directly referring to the bundle / OCI image in a container registry rather than a package name of an extension in a catalog containing the image in order to simplify testing and delivering hotfixes.</p>"},{"location":"olmv1_roadmap/#f19-extension-constraints-p1","title":"F19 - Extension constraints (P1)","text":"<p>OLM needs to allow extensions to specify constraints towards aspects of the running cluster and other currently installed or future extensions. Aspects of the running cluster should include software version, resource utilization, overall resource availability and state of configuration settings. These constraints need to be evaluated when extensions are attempted to be installed or updated.</p>"},{"location":"olmv1_roadmap/#f20-extension-health-p1","title":"F20 - Extension health (P1)","text":"<p>OLM needs to be able to report the overall health state of the extension on a cluster along the following set of aspects: presence of all required objects from the extension bundle, health of all components that have a liveness / readiness endpoint, presence and health of all other extensions the extension in question has a dependency on as well as evaluation of all additional constraints from F19. An extension that was forced to install despite missing / unhealthy dependencies and violated constraints has a reduced health scope down to the liveness / readiness endpoint.</p>"},{"location":"olmv1_roadmap/#f21-custom-extension-health-p3","title":"F21 - Custom extension health (P3)","text":"<p>OLM should provide a way for extensions to report an aggregate health state with custom logic. This should align with other communications channels that are also used for extensions to declare update readiness (F25). This way extensions can report health more accurately than what OLM reports today based on simple readiness of the extension controller pod. Clients like graphical consoles should be able to make use of this to supply additional overall health states of extensions that provide some form of control plane by the user of other extensions.</p>"},{"location":"olmv1_roadmap/#f22-best-effort-resolution-p2","title":"F22 - Best effort resolution (P2)","text":"<p>OLM should always try its best to resolve installation and update requests with the currently available and healthy set catalogs to resolve against. Intermittently or permanently failed catalogs should not block resolution for installation and updates. Fulfilling user requests is valued higher than determinism of results.</p>"},{"location":"olmv1_roadmap/#f23-opt-in-to-fallback-rollback-p2","title":"F23 - Opt-in to fallback / rollback (P2)","text":"<p>OLM should allow extension developers to specify whether or not it is safe to rollback from a particular current version of the extension to an author-specified previous version, once an extension update has passed pre-flights checks in F10 but subsequently failed to become available or carry out a migration. In these cases OLM should allow the administrator to downgrade the extension to the specific previous version. OLM should also respect this downgrade path when conducting updates that fail and use it to fail back to the previous version of the extension indicating that the downgrade path is supported. Extension uptime is an important goal of OLM.</p>"},{"location":"olmv1_roadmap/#f24-extension-overrides-p2","title":"F24 - Extension Overrides (P2)","text":"<p>Components deployed as part of extensions will require user-provided modifications at runtime in order to aid features like placement, networking configuration, proxy configuration etc. that require customization of volume mounts, annotations, labels, environment variables, taints, tolerations, node selectors, affinity, and resources. OLM should support accepting these customizations to the extension as input from the user prior or after the installation of an extension and apply them to applicable objects such as Deployments, StatefulSets, ReplicatSets.</p>"},{"location":"olmv1_roadmap/#f25-extension-controlled-update-readiness-p2","title":"F25 - Extension-controlled Update Readiness (P2)","text":"<p>Extensions should be able to control their readiness for updates. An extension could be on a critical path or in a state where updates would lead to disruption or worst-case: outages. OLM should respect these update readiness signals and allow the extension to signal readiness differentiated to what nature the update is based on semantic versioning rules, i.e. patch updates vs. minor or major updates. Once the signal is encountered, OLM should block the update until the signal disappears.</p>"},{"location":"olmv1_roadmap/#f26-canary-style-rollouts-p3","title":"F26 - Canary Style Rollouts (P3)","text":"<p>OLM should have an opinion about how administrators can carry out roll outs of new extension versions that coexist with already installed versions of the extension, in particular if the extension only ships a controller. While conflicting CRDs cannot co-exist, controllers that only selectively reconcile objects (Ingress controller pattern) can. OLM should support these deployment styles while ensuring integrity of cluster-level extensions like CRDs.</p>"},{"location":"olmv1_roadmap/#f27-pluggable-certificate-management-p2","title":"F27 - Pluggable certificate management (P2)","text":"<p>OLM should rely on other controllers to create and lifecycle TLS certificates required to drive the functionality of certain extensions, like webhooks that need to trust /  need to be trusted by the cluster's API server. OLM should not implement any certificate handling itself. In a first implementation support should be established for cert-manager.</p>"},{"location":"olmv1_roadmap/#f28-provided-service-versions-p2","title":"F28 - Provided service versions (P2)","text":"<p>Workload-based extensions can offer multiple services (a.k.a. operands) and their respective versions. Extension admins need to see which operand versions each extension supports by the extension version. Extension admins are guaranteed to install or upgrade an extension that supports their desired operand version(s). Extension authors can list supported operand versions and have guarantees that they can list dependencies that support the necessary operand version(s). When mirroring a catalog, mirroring users can select a subset of the related images to mirror, based on desired operand version(s).</p>"},{"location":"olmv1_roadmap/#behavioral-requirements","title":"Behavioral Requirements","text":"<p>Priority Rating: 1 highest, 2 medium, 3 lower (ex. P2 = Medium Priority)</p>"},{"location":"olmv1_roadmap/#b1-single-api-control-surface-p1","title":"B1 - Single API control surface (P1)","text":"<p>While the underlying implementation of the functional requirements can be carried out by different controllers with different APIs, to the administrative and non-administrative users there should be a single, cluster-level API that represents an installed extension with all high level controls described in / required by F4, F7, F8, F10, F13, F14, F15, F16, F17, F18, F21, F22, F23 and F24.</p>"},{"location":"olmv1_roadmap/#b2-gitops-friendly-api-surface-p1","title":"B2 - GitOps-friendly API surface (P1)","text":"<p>In many cases OLM APIs will not be used by a human via a CLI or GUI interactively but declaratively through a GitOps pipeline. This means the primary OLM API to lifecycle an extension cannot leak abstractions to other APIs for initial deployment or reconfiguration. Modifications must not require conditional lookup or modifications of other objects on the cluster that are created as part of the declarative intent stored in git in the form of YAML manifest files.</p>"},{"location":"olmv1_roadmap/#b3-declarative-api-p1","title":"B3 - Declarative API (P1)","text":"<p>As an extension itself, OLMs API controls have to allow for operations to be carried out solely declaratively. This mandates continuous reconciliation and eventual consistency of desired state. OLM should not conduct one-off operations. OLM should not require either clean up of failed operations or restating intent to retry a failed operation (with the exception of F11).</p>"},{"location":"olmv1_roadmap/#b4-event-trail-p2","title":"B4 - Event trail (P2)","text":"<p>OLM should make heavy use of Kubernetes events to leave an audit trail of tasks and actions carried out on the cluster. For expected failure scenarios administrators should not need to consult the OLM controller logs for debugging but solely rely on events and status conditions (see also B6).</p>"},{"location":"olmv1_roadmap/#b5-force-overrides-p1","title":"B5 - Force overrides (P1)","text":"<p>While OLM has a lot of opinions about safe operations with cluster extensions they do not apply all the time since OLM cannot possibly foresee how extensions behave at runtime. It needs to yield to the user in cases where they have more certainty about what's going to happen based on their background knowledge of the cluster or the extension. It should offer ways to force-override decisions that OLM made that block the user from proceeding in a certain direction, especially in the areas of extension installation, removal and updates.</p>"},{"location":"olmv1_roadmap/#b6-human-readable-status-extensions-information-p2","title":"B6 - Human-readable status extensions information (P2)","text":"<p>Whenever OLM is in the process of or having reached or failed to reach a desired state it needs to update the user about what is happening / what has happened without assuming knowledge about OLM internal or implementation details.</p>"},{"location":"olmv1_roadmap/#b7-scalability-resource-consumption-p1","title":"B7 - Scalability &amp; Resource consumption (P1)","text":"<p>OLM is used on clusters with hundreds to thousands of namespaces and tenants. Its API controls, specifically for F2 and F12 need to be built in such a way that resource consumption scales linearly with usage and cluster size and the overall resource usage envelope stays within manageable bounds that does not put the cluster stability, especially that of the API server at risk. System memory especially is a scarce resource.</p>"},{"location":"olmv1_roadmap/#compatibility-requirements","title":"Compatibility Requirements:","text":""},{"location":"olmv1_roadmap/#c1-compatibility-with-existing-extensions-p1","title":"C1 - Compatibility with existing extensions  (P1)","text":"<p>OLM should be able to manage extensions packaged with the current bundle format in the way described by the functional and behavior requirements when the bundle supports AllNamespace installation mode.</p>"},{"location":"olmv1_roadmap/#c2-compatibility-with-existing-catalogs-p1","title":"C2 - Compatibility with existing catalogs (P1)","text":"<p>OLM should be able to retrieve and update extensions that adhere to C1 from the currently supported catalog formats (File-based catalogs).</p>"},{"location":"olmv1_roadmap/#c3-incompatibility-with-existing-extensions-p1","title":"C3 - Incompatibility with existing extensions (P1)","text":"<p>OLM 1.0 does not support managing bundles or extension versions that do not support AllNamespace installation mode with the new set of APIs or flows</p>"},{"location":"olmv1_roadmap/#assumptions","title":"Assumptions","text":"<ul> <li> <p>No additional tenancy model will be introduced at the control plane / API layer of Kubernetes upstream</p> </li> <li> <p>kcp doesn\u2019t fundamentally change OLMs role and responsibilities around managing extensions (at least initially)</p> </li> <li> <p>OLM will move to a descoped, cluster-wide singleton model for cluster extensions, extension management isn\u2019t namespaced</p> </li> </ul>"},{"location":"olmv1_roadmap/#constraints","title":"Constraints","text":"<ul> <li>Only extension bundles with \u201cAllNamespace\u201d mode installation support can be lifecycled with the new APIs / flows in OLM 1.0</li> </ul>"},{"location":"olmv1_roadmap/#dependencies","title":"Dependencies","text":"<ul> <li>\"F13 - Extension permissions management (P1)\" and \"F12 - Installed extension discovery (P1)\" will land prior to the GA of OLM 1.0 to unblock most extensions that do not support AllNamespace installation mode today.</li> </ul>"},{"location":"olmv1_roadmap/#migration","title":"Migration","text":"<ul> <li> <p>A new set of APIs is introduced in parallel to the existing set of APIs</p> </li> <li> <p>Users opt-in to the new set of APIs, potentially resulting in a reinstall of their extension if required</p> </li> <li> <p>Extensions that are shipped with the current bundle format with AllNamespace mode can simply be reused with the new set of APIs and controls</p> </li> <li> <p>Extensions that do not support AllNamespace mode cannot be managed with the new APIs</p> </li> <li> <p>Migration scripting is provided to mass-convert existing installed extensions (\u201cSubscription\u201d / \u201cOperatorGroup\u201d objects) on existing clusters to the new OLM 1.0 model assuming they are compatible</p> </li> <li> <p>Extension authors that are also SRE/Managed PaaS administrators are incentivized to make their extension compatible with the requirements of OLM 1.0 to reap the operational benefits</p> </li> </ul>"},{"location":"olmv1_roadmap/#todo","title":"TODO","text":"<ul> <li>Definition of \"extension\"</li> <li>Does OLM become ELM?  Does this provide of provisioning bundles that do not add APIs?</li> </ul>"},{"location":"Tasks/adding-a-catalog/","title":"Adding a catalog of extensions to a cluster","text":"<p>Extension authors can publish their products in catalogs. ClusterCatalogs are curated collections of Kubernetes extensions, such as Operators. Cluster administrators can add these ClusterCatalogs to their cluster. Cluster administrators can enable polling to get over-the-air updates to ClusterCatalogs when extension authors publish changes such as bug fixes and new features.</p> <p>For example, the Kubernetes community Operators catalog is a catalog of curated extensions that is developed by the Kubernetes community. You can see the available extensions at Operatorhub.io. This catalog is distributed as an image quay.io/operatorhubio/catalog that can be installed on clusters.</p>"},{"location":"Tasks/adding-a-catalog/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a Kubernetes cluster, for example <code>kind</code>, using an account with <code>cluster-admin</code> permissions</li> <li>Operator Controller installed on the cluster</li> <li>Catalogd installed on the cluster</li> <li>Kubernetes CLI (<code>kubectl</code>) installed on your workstation</li> </ul>"},{"location":"Tasks/adding-a-catalog/#procedure","title":"Procedure","text":"<ol> <li> <p>Create a catalog custom resource (CR):</p> clustercatalog_cr.yaml<pre><code>apiVersion: catalogd.operatorframework.io/v1alpha1\nkind: ClusterCatalog\nmetadata:\n  name: operatorhubio\nspec:\n  source:\n    type: image\n    image:\n      ref: &lt;catalog_image&gt;\n      pollInterval: &lt;poll_interval_duration&gt;\n</code></pre> <code>catalog_name</code> Specifies the image reference for the catalog you want to install, such as <code>quay.io/operatorhubio/catalog:latest</code>. <code>poll_interval_duration</code> Specifies the interval for polling the remote registry for newer image digests.     The default value is <code>24h</code>.     Valid units include seconds (<code>s</code>), minutes (<code>m</code>), and hours (<code>h</code>).     To disable polling, set a zero value, such as <code>0s</code>. Example `operatorhubio.yaml` CR<pre><code>apiVersion: catalogd.operatorframework.io/v1alpha1\nkind: ClusterCatalog\nmetadata:\n  name: operatorhub\nspec:\n  source:\n    type: image\n    image:\n      ref: quay.io/operatorhubio/catalog:latest\n      pollInterval: 1h\n</code></pre> </li> <li> <p>Apply the ClusterCatalog CR:</p> <pre><code>kubectl apply -f &lt;clustercatalog_cr&gt;.yaml\n</code></pre> Example output<pre><code>clustercatalog.catalogd.operatorframework.io/operatorhubio created\n</code></pre> </li> </ol>"},{"location":"Tasks/adding-a-catalog/#verification","title":"Verification","text":"<ul> <li> <p>Run the following commands to verify the status of your catalog:</p> <ul> <li> <p>Check if your catalog is available on the cluster:</p> <pre><code>kubectl get clustercatalog\n</code></pre> Example output<pre><code>NAME            PHASE   AGE\noperatorhubio           9s\n</code></pre> </li> <li> <p>Check the status of your catalog:</p> <pre><code>kubectl describe clustercatalog\n</code></pre> Example output<pre><code>Name:         operatorhubio\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  catalogd.operatorframework.io/v1alpha1\nKind:         ClusterCatalog\nMetadata:\n  Creation Timestamp:  2024-03-12T19:34:50Z\n  Finalizers:\n    catalogd.operatorframework.io/delete-server-cache\n  Generation:        2\n  Resource Version:  6469\n  UID:               2e2778cb-dda6-4645-96b7-992e8dd37503\nSpec:\n  Source:\n    Image:\n      Poll Interval:  15m0s\n      Ref:            quay.io/operatorhubio/catalog:latest\n    Type:             image\nStatus:\n  Conditions:\n    Last Transition Time:  2024-03-12T19:35:34Z\n    Message:\n    Reason:                UnpackSuccessful\n    Status:                True\n    Type:                  Unpacked\n  Content URL:             https://catalogd-catalogserver.olmv1-system.svc/catalogs/operatorhubio/all.json\n  Observed Generation:     2\n  Phase:                   Unpacked\n  Resolved Source:\n    Image:\n      Last Poll Attempt:  2024-03-12T19:35:26Z\n      Ref:                quay.io/operatorhubio/catalog:latest\n      Resolved Ref:       quay.io/operatorhubio/catalog@sha256:dee29aaed76fd1c72b654b9bc8bebc4b48b34fd8d41ece880524dc0c3c1c55ec\n    Type:                 image\nEvents:                   &lt;none&gt;\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Tasks/explore-available-packages/","title":"Finding extensions to install","text":"<p>After you add a catalog of extensions to your cluster, you must port forward your catalog as a service. Then you can query the catalog by using <code>curl</code> commands and the <code>jq</code> CLI tool to find extensions to install.</p>"},{"location":"Tasks/explore-available-packages/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have added a ClusterCatalog of extensions, such as OperatorHub.io, to your cluster.</li> <li>You have installed the <code>jq</code> CLI tool.</li> </ul> <p>Note: By default, Catalogd is installed with TLS enabled for the catalog webserver. The following examples will show this default behavior, but for simplicity's sake will ignore TLS verification in the curl commands using the <code>-k</code> flag.</p>"},{"location":"Tasks/explore-available-packages/#procedure","title":"Procedure","text":"<ol> <li> <p>Port forward the catalog server service:</p> <pre><code>kubectl -n olmv1-system port-forward svc/catalogd-catalogserver 8443:443\n</code></pre> </li> <li> <p>Return a list of all the extensions in a catalog:     <pre><code>curl -k https://localhost:8443/catalogs/operatorhubio/all.json | jq -s '.[] | select(.schema == \"olm.package\") | .name'\n</code></pre></p> Success Example output<pre><code>\"ack-acm-controller\"\n\"ack-acmpca-controller\"\n\"ack-apigatewayv2-controller\"\n\"ack-applicationautoscaling-controller\"\n\"ack-cloudfront-controller\"\n\"ack-cloudtrail-controller\"\n\"ack-cloudwatch-controller\"\n\"ack-cloudwatchlogs-controller\"\n\"ack-dynamodb-controller\"\n\"ack-ec2-controller\"\n\"ack-ecr-controller\"\n\"ack-ecs-controller\"\n\"ack-efs-controller\"\n\"ack-eks-controller\"\n\"ack-elasticache-controller\"\n\"ack-emrcontainers-controller\"\n\"ack-eventbridge-controller\"\n\"ack-iam-controller\"\n\"ack-kafka-controller\"\n\"ack-keyspaces-controller\"\n\"ack-kinesis-controller\"\n\"ack-kms-controller\"\n\"ack-lambda-controller\"\n\"ack-memorydb-controller\"\n\"ack-mq-controller\"\n\"ack-networkfirewall-controller\"\n\"ack-opensearchservice-controller\"\n\"ack-pipes-controller\"\n\"ack-prometheusservice-controller\"\n\"ack-rds-controller\"\n\"ack-route53-controller\"\n\"ack-route53resolver-controller\"\n\"ack-s3-controller\"\n\"ack-sagemaker-controller\"\n\"ack-secretsmanager-controller\"\n\"ack-sfn-controller\"\n\"ack-sns-controller\"\n\"ack-sqs-controller\"\n\"aerospike-kubernetes-operator\"\n\"airflow-helm-operator\"\n\"aiven-operator\"\n\"akka-cluster-operator\"\n\"alvearie-imaging-ingestion\"\n\"anchore-engine\"\n\"apch-operator\"\n\"api-operator\"\n\"api-testing-operator\"\n\"apicast-community-operator\"\n\"apicurio-registry\"\n\"apimatic-kubernetes-operator\"\n\"app-director-operator\"\n\"appdynamics-operator\"\n\"application-services-metering-operator\"\n\"appranix\"\n\"aqua\"\n\"argocd-operator\"\n...\n</code></pre> <p>Important</p> <p>Currently, OLM 1.0 does not support the installation of extensions that use webhooks or that target a single or specified set of namespaces.</p> <ul> <li> <p>Return list of packages that support <code>AllNamespaces</code> install mode and do not use webhooks:</p> <pre><code>curl -k https://localhost:8443/catalogs/operatorhubio/all.json | jq -c 'select(.schema == \"olm.bundle\") | {\"package\":.package, \"version\":.properties[] | select(.type == \"olm.bundle.object\").value.data | @base64d | fromjson | select(.kind == \"ClusterServiceVersion\" and (.spec.installModes[] | select(.type == \"AllNamespaces\" and .supported == true) != null) and .spec.webhookdefinitions == null).spec.version}'\n</code></pre> Success Example output<pre><code>{\"package\":\"ack-acm-controller\",\"version\":\"0.0.12\"}\n{\"package\":\"ack-acmpca-controller\",\"version\":\"0.0.5\"}\n{\"package\":\"ack-apigatewayv2-controller\",\"version\":\"1.0.7\"}\n{\"package\":\"ack-applicationautoscaling-controller\",\"version\":\"1.0.11\"}\n{\"package\":\"ack-cloudfront-controller\",\"version\":\"0.0.9\"}\n{\"package\":\"ack-cloudtrail-controller\",\"version\":\"1.0.8\"}\n{\"package\":\"ack-cloudwatch-controller\",\"version\":\"0.0.3\"}\n{\"package\":\"ack-cloudwatchlogs-controller\",\"version\":\"0.0.4\"}\n{\"package\":\"ack-dynamodb-controller\",\"version\":\"1.2.9\"}\n{\"package\":\"ack-ec2-controller\",\"version\":\"1.2.4\"}\n{\"package\":\"ack-ecr-controller\",\"version\":\"1.0.12\"}\n{\"package\":\"ack-ecs-controller\",\"version\":\"0.0.4\"}\n{\"package\":\"ack-efs-controller\",\"version\":\"0.0.5\"}\n{\"package\":\"ack-eks-controller\",\"version\":\"1.3.3\"}\n{\"package\":\"ack-elasticache-controller\",\"version\":\"0.0.29\"}\n{\"package\":\"ack-emrcontainers-controller\",\"version\":\"1.0.8\"}\n{\"package\":\"ack-eventbridge-controller\",\"version\":\"1.0.6\"}\n{\"package\":\"ack-iam-controller\",\"version\":\"1.3.6\"}\n{\"package\":\"ack-kafka-controller\",\"version\":\"0.0.4\"}\n{\"package\":\"ack-keyspaces-controller\",\"version\":\"0.0.11\"}\n...\n</code></pre> </li> </ul> </li> <li> <p>Inspect the contents of an extension's metadata:</p> <pre><code>curl -k https://localhost:8443/catalogs/operatorhubio/all.json | jq -s '.[] | select( .schema == \"olm.package\") | select( .name == \"&lt;package_name&gt;\")'\n</code></pre> <code>package_name</code> Specifies the name of the package you want to inspect. Success Example output<pre><code>{\n  \"defaultChannel\": \"stable-v6.x\",\n  \"icon\": {\n    \"base64data\": \"PHN2ZyB4bWxucz0ia...\n    \"mediatype\": \"image/svg+xml\"\n  },\n  \"name\": \"cockroachdb\",\n  \"schema\": \"olm.package\"\n}\n</code></pre> </li> </ol>"},{"location":"Tasks/explore-available-packages/#additional-resources","title":"Additional resources","text":"<ul> <li>Catalog queries</li> </ul>"},{"location":"Tasks/installing-an-extension/","title":"Installing an extension from a catalog","text":"<p>In Operator Lifecycle Manager (OLM) 1.0, Kubernetes extensions are scoped to the cluster. After you add a catalog to your cluster, you can install an extension by creating a custom resource (CR) and applying it.</p> <p>Important</p> <p>Currently, extensions that use webhooks or target a single or specified set of namespaces cannot be installed. Extensions must not include webhooks and must use the <code>AllNamespaces</code> install mode.</p>"},{"location":"Tasks/installing-an-extension/#prerequisites","title":"Prerequisites","text":"<ul> <li>The <code>jq</code> CLI tool is installed.</li> <li>You have added a catalog to your cluster.</li> </ul>"},{"location":"Tasks/installing-an-extension/#procedure","title":"Procedure","text":"<ol> <li> <p>Create a CR for the Kubernetes extension you want to install:</p> Example CR<pre><code>apiVersion: olm.operatorframework.io/v1alpha1\nkind: ClusterExtension\nmetadata:\n  name: &lt;extension_name&gt;\nspec:\n  packageName: &lt;package_name&gt;\n  channel: &lt;channel&gt;\n  version: \"&lt;version&gt;\"\n</code></pre> <code>extension_name</code> Specifies a custom name for the Kubernetes extension you want to install, such as <code>my-camel-k</code>. <code>package_name</code> Specifies the name of the package you want to install, such as <code>camel-k</code>. <code>channel</code> Optional: Specifies the extension's channel, such as <code>stable</code> or <code>candidate</code>. <code>version</code> Optional: Specifies the version or version range you want installed, such as <code>1.3.1</code> or <code>\"&lt;2\"</code>.  If you use a comparison string to define a version range, the string must be surrounded by double quotes (<code>\"</code>). <p>Warning</p> <p>Currently, the following limitations affect the installation of extensions:</p> <ul> <li>If mulitple catalogs are added to a cluster, you cannot specify a catalog when you install an extension.</li> <li>OLM 1.0 requires that all of the extensions have unique bundle and package names for dependency resolution.</li> </ul> <p>As a result, if two catalogs have an extension with the same name, the installation might fail or lead to an unintended outcome. For example, the first extension that matches might install successfully and finish without searching for a match in the second catalog.</p> </li> <li> <p>Apply the CR to the cluster:</p> <pre><code>kubectl apply -f &lt;cr_name&gt;.yaml\n</code></pre> Success Example output<pre><code>clusterextension.olm.operatorframework.io/camel-k created\n</code></pre> </li> </ol>"},{"location":"Tasks/installing-an-extension/#verification","title":"Verification","text":"<ul> <li> <p>Describe the installed extension:</p> <pre><code>kubectl describe clusterextensions\n</code></pre> Success Example output<pre><code>Name:         my-camel-k\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  olm.operatorframework.io/v1alpha1\nKind:         ClusterExtension\nMetadata:\n  Creation Timestamp:  2024-03-15T15:03:47Z\n  Generation:          1\n  Resource Version:    7691\n  UID:                 d756879f-217d-4ebe-85b1-8427bbb2f1df\nSpec:\n  Package Name:               camel-k\n  Upgrade Constraint Policy:  Enforce\nStatus:\n  Conditions:\n    Last Transition Time:     2024-03-15T15:03:50Z\n    Message:                  resolved to \"quay.io/operatorhubio/camel-k@sha256:d2b74c43ec8f9294450c9dcf2057be328d0998bb924ad036db489af79d1b39c3\"\n    Observed Generation:      1\n    Reason:                   Success\n    Status:                   True\n    Type:                     Resolved\n    Last Transition Time:     2024-03-15T15:04:13Z\n    Message:                  installed from \"quay.io/operatorhubio/camel-k@sha256:d2b74c43ec8f9294450c9dcf2057be328d0998bb924ad036db489af79d1b39c3\"\n    Observed Generation:      1\n    Reason:                   Success\n    Status:                   True\n    Type:                     Installed\n  Installed Bundle Resource:  quay.io/operatorhubio/camel-k@sha256:d2b74c43ec8f9294450c9dcf2057be328d0998bb924ad036db489af79d1b39c3\n  Resolved Bundle Resource:   quay.io/operatorhubio/camel-k@sha256:d2b74c43ec8f9294450c9dcf2057be328d0998bb924ad036db489af79d1b39c3\nEvents:                       &lt;none&gt;\n</code></pre> </li> </ul>"},{"location":"Tasks/uninstall-an-extension/","title":"Deleting an extension","text":"<p>You can uninstall a Kubernetes extension and its associated custom resource definitions (CRD) by deleting the extension's custom resource (CR).</p>"},{"location":"Tasks/uninstall-an-extension/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have an extension installed.</li> </ul>"},{"location":"Tasks/uninstall-an-extension/#procedure","title":"Procedure","text":"<ul> <li> <p>Delete the extension's CR:</p> <pre><code>kubectl delete clusterextensions &lt;extension_name&gt;\n</code></pre> <code>extension_name</code> Specifies the name defined in the <code>metadata.name</code> field of the extension's CR. Example output<pre><code>clusterextension.olm.operatorframework.io \"argocd-operator\" deleted\n</code></pre> </li> </ul>"},{"location":"Tasks/uninstall-an-extension/#verification","title":"Verification","text":"<ul> <li> <p>Verify that the Kubernetes extension is deleted:</p> <pre><code>kubectl get clusterextension.olm.operatorframework.io\n</code></pre> Example output<pre><code>No resources found\n</code></pre> </li> </ul>"},{"location":"drafts/support-watchNamespaces/","title":"Install Modes and WatchNamespaces in OMLv1","text":"<p>Operator Lifecycle Manager (OLM) operates with cluster-admin privileges, enabling it to grant necessary permissions to the Extensions it deploys. For extensions packaged as <code>RegistryV1</code> bundles, it's the responsibility of the authors to specify supported <code>InstallModes</code> in the ClusterServiceVersion (CSV). InstallModes define the operational scope of the extension within the Kubernetes cluster, particularly in terms of namespace availability. The four recognized InstallModes are as follows:</p> <ol> <li>OwnNamespace: This mode allows the extension to monitor and respond to events within its own deployment namespace.</li> <li>SingleNamespace: In this mode, the extension is set up to observe events in a single, specific namespace other than the one it is deployed in.</li> <li>MultiNamespace: This enables the extension to function across multiple specified namespaces.</li> <li>AllNamespaces: Under this mode, the extension is equipped to monitor events across all namespaces within the cluster.</li> </ol> <p>When creating a cluster extension, users have the option to define a list of <code>watchNamespaces</code>. This list determines the specific namespaces within which they intend the operator to operate. The configuration of <code>watchNamespaces</code> must align with the InstallModes supported by the extension as specified by the bundle author. The supported configurations in the order of preference are as follows:</p> Length of <code>watchNamespaces</code> specified through ClusterExtension Allowed values Supported InstallMode in CSV Description 0 (Empty/Unset) - AllNamespaces Extension monitors all namespaces. - OwnNamespace Supported when <code>AllNamespaces</code> is false. Extension only active in its deployment namespace. 1 (Single Entry) <code>\"\"</code> (Empty String) AllNamespaces Extension monitors all namespaces. Entry equals Install Namespace OwnNamespace Extension watches only its install namespace. Entry is a specific namespace (not the Install Namespace) SingleNamespace Extension monitors a single, specified namespace in the spec. &gt;1 (Multiple Entries) Entries are specific, multiple namespaces MultiNamespace Extension monitors each of the specified multiple namespaces in the spec."},{"location":"drafts/upgrade-support/","title":"Upgrade support","text":"<p>This document explains how OLM 1.0 handles upgrades.</p> <p>OLM 1.0 introduces a simplified UX for package authors and package admins to implicitly define upgrade edges via Semantic Versioning.</p> <p>It also introduces an API to enable independently verified upgrades and downgrades.</p>"},{"location":"drafts/upgrade-support/#upgrade-constraint-semantics","title":"Upgrade constraint semantics","text":"<p>As of operator-controller release 0.10.0, OLM 1.0 supports the following upgrade constraint semantics:</p> <ul> <li>Semantic Versioning (Semver)</li> <li>Legacy OLM 0 semantics: the <code>replaces</code>/<code>skips</code>/<code>skipRange</code> directives</li> </ul> <p>The Kubernetes manifests in this repo enable legacy support by default. Cluster admins can control which semantics to use by passing one of the following arguments to the <code>manager</code> binary: * <code>--feature-gates=ForceSemverUpgradeConstraints=true</code> - enable Semver * <code>--feature-gates=ForceSemverUpgradeConstraints=false</code> - disable Semver, use legacy semantics</p> <p>For example, to enable Semver update the <code>controller-manager</code> Deployment manifest to include the following argument:</p> <pre><code>- command:\n  - /manager\n  args:\n  - --feature-gates=ForceSemverUpgradeConstraints=true\n  image: controller:latest\n</code></pre> <p>In a future release, it is planned to remove the <code>ForceSemverUpgradeConstraints</code> feature gate and allow package authors to specify upgrade constraint semantics at the catalog level.</p>"},{"location":"drafts/upgrade-support/#upgrades","title":"Upgrades","text":"<p>OLM supports Semver to provide a simplified way for package authors to define compatible upgrades. According to the Semver standard, releases within a major version (e.g. <code>&gt;=1.0.0 &lt;2.0.0</code>) must be compatible. As a result, package authors can publish a new package version following the Semver specification, and OLM assumes compatibility. Package authors do not have to explicitly define upgrade edges in the catalog.</p> <p>[!NOTE] Currently, OLM 1.0 does not support automatic upgrades to the next major version. You must manually verify and perform major version upgrades. For more information about major version upgrades, see Manually verified upgrades and downgrades.</p>"},{"location":"drafts/upgrade-support/#upgrades-within-the-major-version-zero","title":"Upgrades within the major version zero","text":"<p>According to the Semver specification, a major version zero release is for initial development. It is assumed that breaking changes might be introduced at any time. As a result, the following special conditions apply to upgrades within a major version zero release:</p> <ul> <li>You cannot automatically upgrade from one patch version to another when both major and minor versions are <code>0</code>. For example, automatic upgrades within the following version range are not allowed: <code>&gt;= 0.0.1 &lt;0.1.0</code>.</li> <li>You cannot automatically upgrade from one minor version to another minor version within the major version zero. For example, no upgrades from <code>0.1.0</code> to <code>0.2.0</code>. However, you can upgrade from patch versions. For example, upgrades are possible in ranges <code>&gt;= 0.1.0 &lt;0.2.0</code>, <code>&gt;= 0.2.0 &lt;0.3.0</code>, <code>&gt;= 0.3.0 &lt;0.4.0</code>, and so on.</li> </ul> <p>You must verify and perform upgrades manually in cases where automatic upgrades are blocked.</p>"},{"location":"drafts/upgrade-support/#manually-verified-upgrades-and-downgrades","title":"Manually verified upgrades and downgrades","text":"<p>Warning: If you want to force an upgrade manually, you must thoroughly verify the outcome before applying any changes to production workloads. Failure to test and verify the upgrade might lead to catastrophic consequences such as data loss.</p> <p>As a package admin, if you must upgrade or downgrade to version that might be incompatible with the currently installed version, you can set the <code>.spec.upgradeConstraintPolicy</code> field to <code>Ignore</code> on the relevant <code>ClusterExtension</code> resource.</p> <p>If you set the field to <code>Ignore</code>, no upgrade constraints are set on the package. As a result, you can change the version to any version available in the catalogs for a given package.</p> <p>Example <code>ClusterExtension</code> with <code>.spec.upgradeConstraintPolicy</code> field set to <code>Ignore</code>:</p> <pre><code>apiVersion: olm.operatorframework.io/v1alpha1\nkind: ClusterExtension\nmetadata:\n  name: extension-sample\nspec:\n  packageName: argocd-operator\n  version: 0.6.0\n  upgradeConstraintPolicy: Ignore\n</code></pre>"},{"location":"drafts/version-ranges/","title":"Extension version ranges","text":"<p>This document explains how to specify a version range to install or update an extension with OLM 1.0.</p> <p>You define a version range in a ClusterExtension's custom resource (CR) file.</p>"},{"location":"drafts/version-ranges/#specifying-a-version-range-in-the-cr","title":"Specifying a version range in the CR","text":"<p>If you specify a version range in the ClusterExtension's CR, OLM 1.0 installs or updates the latest version of the extension that can be resolved within the version range. The resolved version is the latest version of the extension that satisfies the dependencies and constraints of the extension and the environment. Extension updates within the specified range are automatically installed if they can be resolved successfully. Updates are not installed if they are outside of the specified range or if they cannot be resolved successfully.</p> <p>For more information about dependency and constraint resolution in OLM 1.0, see the Deppy introduction</p>"},{"location":"drafts/version-ranges/#comparisons","title":"Comparisons","text":"<p>You define a version range by adding a comparison string to the <code>spec.version</code> field. A comparison string is composed of a list of comma or space separated values and one or more comparison operators. You can add an additional comparison string by including an OR (<code>||</code>) operator between the strings.</p>"},{"location":"drafts/version-ranges/#basic-comparisons","title":"Basic comparisons","text":"Operator Definition <code>=</code> equal (not aliased to an operator) <code>!=</code> not equal <code>&gt;</code> greater than <code>&lt;</code> less than <code>&gt;=</code> greater than or equal to <code>&lt;=</code> less than or equal to"},{"location":"drafts/version-ranges/#range-comparisons","title":"Range comparisons","text":"<p>To specify a version range, use a range comparison similar to the following example:</p> <pre><code>version: &gt;=3.0, &lt;3.6\n</code></pre>"},{"location":"drafts/version-ranges/#wildcards-in-comparisons","title":"Wildcards in comparisons","text":"<p>You can use the <code>x</code>, <code>X</code>, and <code>*</code> characters as wildcard characters in all comparison operations. If you use a wildcard character with the <code>=</code> operator, you define a patch level comparision. This is equivalent to making a tilde range comparison.</p> <p>Example comparisons with wildcard characters | Comparison | Equivalent          | |------------|---------------------| | <code>1.2.x</code>    | <code>&gt;= 1.2.0, &lt; 1.3.0</code> | | <code>&gt;= 1.2.x</code> | <code>&gt;= 1.2.0</code>          | | <code>&lt;= 2.x</code>   | <code>&lt; 3</code>               | | <code>*</code>        | <code>&gt;= 0.0.0</code>          |</p>"},{"location":"drafts/version-ranges/#patch-release-or-tilde-range-comparison","title":"Patch release or tilde (<code>~</code>) range comparison","text":"<p>You can use the tilde (<code>~</code>) operator to make patch release comparisons. This is useful when you want to specify a minor version up to the next major version.</p> <p>Example patch release comparisons | Comparison | Equivalent          | |------------|---------------------| | <code>~1.2.3</code>   | <code>&gt;= 1.2.3, &lt; 1.3.0</code> | | <code>~1</code>       | <code>&gt;= 1, &lt;2</code>          | | <code>~2.3</code>     | <code>&gt;= 2.3, &lt; 2.4</code>     | | <code>~1.2.x</code>   | <code>&gt;= 1.2.0, &lt; 1.3.0</code> | | <code>~1.x</code>     | <code>&gt;= 1, &lt; 2</code>         |</p>"},{"location":"drafts/version-ranges/#major-release-or-caret-range-comparisons","title":"Major release or caret (<code>^</code>) range comparisons","text":"<p>You can use the caret (<code>^</code>) operator to make major release comparisons after a stable, <code>1.0.0</code>, version is published. If you make a major release comparison before a stable version is published, minor versions define the API stability level.</p> <p>Example major release comparisons</p> Comparison Equivalent <code>^1.2.3</code> <code>&gt;= 1.2.3, &lt; 2.0.0``&gt;= 1.2.3, &lt; 2.0.0</code> <code>^1.2.x</code> <code>&gt;= 1.2.0, &lt; 2.0.0</code> <code>^2.3</code> <code>&gt;= 2.3, &lt; 3</code> <code>^2.x</code> <code>&gt;= 2.0.0, &lt; 3</code> <code>^0.2.3</code> <code>&gt;=0.2.3 &lt;0.3.0</code> <code>^0.2</code> <code>&gt;=0.2.0 &lt;0.3.0</code> <code>^0.0.3</code> <code>&gt;=0.0.3 &lt;0.0.4</code> <code>^0.0</code> <code>&gt;=0.0.0 &lt;0.1.0</code> <code>^0</code> <code>&gt;=0.0.0 &lt;1.0.0</code>"},{"location":"refs/catalog-queries/","title":"Catalog queries","text":"<p>Note: By default, Catalogd is installed with TLS enabled for the catalog webserver. The following examples will show this default behavior, but for simplicity's sake will ignore TLS verification in the curl commands using the <code>-k</code> flag.</p> <p>You can use the <code>curl</code> command with <code>jq</code> to query catalogs that are installed on your cluster.</p> Query syntax<pre><code>curl -k https://localhost:8443/catalogs/operatorhubio/all.json | &lt;query&gt;\n</code></pre>"},{"location":"refs/catalog-queries/#package-queries","title":"Package queries","text":"Available packages in a catalog <pre><code>jq -s '.[] | select( .schema == \"olm.package\")\n</code></pre> Packages that support <code>AllNamespaces</code> install mode and do not use webhooks <pre><code>jq -c 'select(.schema == \"olm.bundle\") | {\"package\":.package, \"version\":.properties[] | select(.type == \"olm.bundle.object\").value.data |  @base64d | fromjson | select(.kind == \"ClusterServiceVersion\" and (.spec.installModes[] | select(.type == \"AllNamespaces\" and .supported == true) != null) and .spec.webhookdefinitions == null).spec.version}'\n</code></pre> Package metadata <pre><code>jq -s '.[] | select( .schema == \"olm.package\") | select( .name == \"&lt;package_name&gt;\")'\n</code></pre> Catalog blobs in a package <pre><code>jq -s '.[] | select( .package == \"&lt;package_name&gt;\")'\n</code></pre>"},{"location":"refs/catalog-queries/#channel-queries","title":"Channel queries","text":"Channels in a package <pre><code>jq -s '.[] | select( .schema == \"olm.channel\" ) | select( .package == \"&lt;package_name&gt;\") | .name'\n</code></pre> Versions in a channel <pre><code>jq -s '.[] | select( .package == \"&lt;package_name&gt;\" ) | select( .schema == \"olm.channel\" ) | select( .name == \"&lt;channel_name&gt;\" ) | .entries | .[] | .name'\n</code></pre> Latest version in a channel and upgrade path <pre><code>jq -s '.[] | select( .schema == \"olm.channel\" ) | select ( .name == \"&lt;channel&gt;\") | select( .package == \"&lt;package_name&gt;\")'\n</code></pre>"},{"location":"refs/catalog-queries/#bundle-queries","title":"Bundle queries","text":"Bundles in a package <pre><code>jq -s '.[] | select( .schema == \"olm.bundle\" ) | select( .package == \"&lt;package_name&gt;\") | .name'\n</code></pre> Bundle dependencies and available APIs <pre><code>jq -s '.[] | select( .schema == \"olm.bundle\" ) | select ( .name == \"&lt;bundle_name&gt;\") | select( .package == \"&lt;package_name&gt;\")'\n</code></pre>"}]}